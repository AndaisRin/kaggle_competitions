# Тренировки по ML в Яндексе
## Обзор программы
Программа "Первые Тренировки по ML" представляет собой цикл лекций и практических задач, нацеленных на закрепление теории в области классического машинного обучения (ML). Участники имеют возможность решать практические задачи.

## Выполненные домашние задания
### 01: [Реализация kNN](https://github.com/AndaisRin/kaggle_competitions/tree/main/05_yandex_ml_trainings/01_knn)
*Навыки:* Python (3.5), numpy, scipy

*Описание:* Реализация алгоритма k ближайших соседей (kNN) без использования внешних библиотек, кроме numpy и scipy.

### 02: [Распределение Лапласа](https://github.com/AndaisRin/kaggle_competitions/tree/main/05_yandex_ml_trainings/02_laplace)
*Навыки:* Python (3.6), numpy

*Описание:* Реализация распределения Лапласа без использования внешних библиотек, кроме numpy. Запрет на использование готовых распределений из scipy.

### 03: [Функции ошибки](https://github.com/AndaisRin/kaggle_competitions/tree/main/05_yandex_ml_trainings/03_derivatives)
*Навыки:* Python (3.8.5), numpy, scipy

*Описание:* Реализация вычисления производных и тестирование с использованием numpy и scipy.

### 04: [Степенной метод](https://github.com/AndaisRin/kaggle_competitions/tree/main/05_yandex_ml_trainings/04_power_iteration)
*Навыки:* Python (3.8.5), numpy

*Описание:* Реализация метода итераций для подсчета собственных значений с использованием numpy.

### 05: [Bagging and OOB](https://github.com/AndaisRin/kaggle_competitions/tree/main/05_yandex_ml_trainings/05_bagging_and_oob)
*Навыки:* Python, numpy

*Описание:* Реализация бэггинга и out-of-bag оценки с использованием numpy.

### 06: [Boosting with MSE](https://github.com/AndaisRin/kaggle_competitions/tree/main/05_yandex_ml_trainings/06_boosting)
*Навыки:* Python, numpy

*Описание:* Реализация бустинга с использованием среднеквадратичной ошибки.

### 07: [Классификация MNIST](https://github.com/AndaisRin/kaggle_competitions/tree/main/05_yandex_ml_trainings/07_mnist_classification)
*Навыки:* Deep Learning, Python, numpy

*Описание:* Решение задачи классификации MNIST с использованием нейронных сетей. Генерация и отправка объекта submission_dict_hw03.npy.

### 08: [Оценка признаков](https://github.com/AndaisRin/kaggle_competitions/tree/main/05_yandex_ml_trainings/08_importances)
*Навыки:* Python, numpy

### 09: [Итоговое задание](https://github.com/AndaisRin/kaggle_competitions/tree/main/05_yandex_ml_trainings/09_final)
*Навыки:* Python, numpy, линейная алгебра, оптимизация

*Описание:* В данной задаче представлена неизвестная зависимость, и цель состоит в построении двух лучших моделей, минимизирующих среднеквадратичную ошибку (MSE).

## Программа лекций
### Лекция 1: Введение в ML + kNN
- Знакомство с основами машинного обучения.
- Формальная постановка задачи обучения с учителем.
- Скалярное произведение, метрика, линейные пространства.
- Метод ближайших соседей (kNN).
### Лекция 2: Линейная регрессия и регуляризация
- Линейная регрессия и ее решения.
- Неустойчивость решения, теорема Гаусса-Маркова.
- Ограничения на вектор параметров.
- L1 и L2 регуляризация и их влияние на решение.
### Лекция 3: Линейная классификация, метод максимального правдоподобия
- Линейные механизмы классификации.
- Отступ, логистическая функция потерь, Hinge loss.
- Правдоподобие, метод максимального правдоподобия.
- Логистическая регрессия и бернуллиевская случайная величина.
### Лекция 4: Решающие деревья, композиции деревьев, Random Forest
- Процедура построения деревьев регрессии и классификации.
- Жадный алгоритм, информационные критерии.
- Бутстрап, бэггинг, случайный лес.
- Особенности решающих деревьев.
### Лекция 5: Градиентный бустинг, тонкости обучения
- Интуитивное объяснение механизма бустинга.
- Градиентный бустинг, ограничения на базовые алгоритмы и функции потерь.
- Различия в применении бустинга и бэггинга.
### Лекция 6: Обзорная лекция по DL
- Основы Deep Learning.
- Нейронные сети как развитие классических моделей машинного обучения.
- Построение моделей с учетом свойств данных.
### Лекция 7: Оценка значимости признаков, обучение без учителя
- Методы оценки значимости признаков: Permutation importance, LIME, Shap values.
- Задачи обучения без учителя: кластеризация, снижение размерности.

## Вывод
Участие в тренировочных заданиях позволило применить знания по классическому машинному обучению, глубже освоить практические навыки работы с алгоритмами и библиотеками. Отработка различных методов, начиная от kNN и линейной регрессии, до бустинга и нейронных сетей, расширила понимание области машинного обучения.
